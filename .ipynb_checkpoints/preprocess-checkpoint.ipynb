{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import  string\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData(object):\n",
    "    def __init__(self, filename, filename_2=None, pair=False):\n",
    "        self.filename = filename\n",
    "        if filename_2:\n",
    "            self.filename_2 = filename_2\n",
    "        self.pair = pair\n",
    "        if pair:\n",
    "            self.text = None\n",
    "        else:\n",
    "            self.source_text = None\n",
    "            self.target_text = None\n",
    "        # store the source sentences and the target sentences\n",
    "        self.source_sentences = []\n",
    "        self.target_sentences = []\n",
    "        # counters for  source sentences and target sentences\n",
    "        self._src_counter = Counter()\n",
    "        self._target_counter = Counter()\n",
    "        self._special_tokens = ['<PAD>', '<GO>', '<EOS>']\n",
    "        \n",
    "    # load the data\n",
    "    def load_text(self, encode='utf-8'):\n",
    "        f = open(self.filename, 'r', encoding=encode)\n",
    "        self.text = f.read()\n",
    "        if self.pair:  # if source and target are within the same file\n",
    "            f.close()\n",
    "            return self.text\n",
    "        self.source_text = self.text\n",
    "        f_2 = open(self.filename_2, 'r')\n",
    "        self.target_text = f_2.read()\n",
    "        f_2.close()\n",
    "        return self.source_text, self.target_text\n",
    "        \n",
    "    # split the data\n",
    "    def _split_data(self):\n",
    "        if self.pair:\n",
    "            lines = self.text.split('\\n')\n",
    "            pair_sentences = [line.split('\\t') for line in lines if line]\n",
    "            self.source_sentences = [line[0] for line in pair_sentences]\n",
    "            self.target_sentences = [line[1] for line in pair_sentences]\n",
    "            return self.source_sentences, self.target_sentences\n",
    "        else:\n",
    "            source_lines = self.source_text.split('\\n')\n",
    "            self.source_sentences = [line for line in source_lines ]\n",
    "            target_lines = self.target_text.split('\\n')\n",
    "            self.target_sentences = [line for line in target_lines ]\n",
    "            return self.source_sentences, self.target_sentences\n",
    "    # clean the data\n",
    "    def clean_data(self):\n",
    "        source_data, target_data = self._split_data()\n",
    "        # surround  punctuations by whitespace\n",
    "        punc_suround = re.compile('[?.!,¿]')\n",
    "        source_data = [re.sub(r\"([?.!,¿])\", r\" \\1 \", phrase) for phrase in source_data]\n",
    "        target_data = [re.sub(r\"([?.!,¿])\", r\" \\1 \", phrase) for phrase in target_data]\n",
    "        source_data = [re.sub(r'[\" \"]+', \" \", phrase) for phrase in source_data]\n",
    "        target_data = [re.sub(r'[\" \"]+', ' ', phrase)for phrase in target_data]\n",
    "        \n",
    "        # transform all letters to latin ascii\n",
    "        source_data = [unicodedata.normalize('NFD', phrase).encode('ascii', 'ignore').decode('UTF-8') for phrase in source_data]\n",
    "        target_data = [unicodedata.normalize('NFD', phrase).encode('ascii', 'ignore').decode('UTF-8') for phrase in target_data]\n",
    "#         source_data = [phrase.decode('UTF-8') for phrase in source_data]\n",
    "#         target_data = [phrase.decode('UTF-8') for phrase in target_data]\n",
    "        #  convert all sentences to lowercase\n",
    "        source_data = [phrase.lower() for  phrase in source_data]\n",
    "        target_data = [phrase.lower() for phrase in target_data]\n",
    "        # delete everything except a-z ,.!?\n",
    "        source_data = [re.sub('[^a-z?,!.¿]', ' ',phrase) for  phrase in source_data]\n",
    "        target_data = [re.sub('[^a-z?,!.¿]', ' ',phrase) for phrase in target_data]\n",
    "        \n",
    "        # add special end token for target sentences #TODO ADDED NEW !!\n",
    "        end_token = '{} '.format(self._special_tokens[-1])\n",
    "        target_data = [phrase + end_token for phrase in target_data]\n",
    "        \n",
    "        return source_data, target_data\n",
    "        \n",
    "        \n",
    "    def save_txt(self,list_phrases, save_path):\n",
    "            \n",
    "#         f = open(save_path, 'w')\n",
    "#         for phrase in list_phrases:\n",
    "#             f.write(phrase)\n",
    "#         f.close()\n",
    "        split_path = save_path.split(\"/\")\n",
    "        print(len(split_path))\n",
    "        if(os.path.exists(\"/\".join(split_path[:- 1])) == False and len(split_path) >= 2):\n",
    "            os.makedirs(\"/\".join(split_path[:- 1]))\n",
    "        with open(save_path, 'w') as saved_file:\n",
    "            for phrase in list_phrases:\n",
    "                print(phrase, file=saved_file)\n",
    "        print('all sentences are saved !')\n",
    "            \n",
    "        \n",
    "    def save_binary(self, list_phrases_src, list_phrases_target, save_path):\n",
    "        \n",
    "        split_path = save_path.split(\"/\")\n",
    "        print(len(split_path))\n",
    "        if(os.path.exists(\"/\".join(split_path[:- 1])) == False and len(split_path) >= 2):\n",
    "            os.makedirs(\"/\".join(split_path[:- 1]))\n",
    "        with open(save_path, 'wb') as file_:\n",
    "            pickle.dump([list_phrases_src, list_phrases_target], file_)\n",
    "\n",
    "        print('file saved')\n",
    "    \n",
    "    def load_binary(self, file_path):\n",
    "        with open(file_path, 'rb') as read_file:\n",
    "            get_data = pickle.load(read_file)\n",
    "        return get_data\n",
    "        \n",
    "    # count the unique tokens\n",
    "    def count_tokens(self, sentences_list, src=True):\n",
    "        for sentence in sentences_list:\n",
    "            if src:\n",
    "                self._src_counter.update(sentence.split())\n",
    "            else:\n",
    "                self._target_counter.update(sentence.split())\n",
    "        print('tokens are counted !')\n",
    "    \n",
    "    # save tokens in a file\n",
    "    def save_tokens(self, file_name, common_words=None, src_counter=True):\n",
    "        if src_counter:\n",
    "            counter = self._src_counter\n",
    "            special_tokens = self._special_tokens\n",
    "        else:\n",
    "            counter = self._target_counter\n",
    "            special_tokens = [self._special_tokens[0]]\n",
    "        \n",
    "        split_path = file_name.split(\"/\")\n",
    "        print(len(split_path))\n",
    "        if(os.path.exists(\"/\".join(split_path[:- 1])) == False and len(split_path) >= 2):\n",
    "            os.makedirs(\"/\".join(split_path[:- 1]))\n",
    "        with open(file_name, 'w') as file_:\n",
    "            for special in special_tokens:\n",
    "                print(special, file=file_)\n",
    "            \n",
    "            if not common_words:\n",
    "                for word in counter.keys():\n",
    "                    print(word, file=file_)\n",
    "                print('all {} tokens are saved'.format(len(counter) + len(special_tokens)))\n",
    "                    \n",
    "            else:\n",
    "                for word, _ in counter.most_common(common_words):\n",
    "                    print(word, file=file_)\n",
    "                print('the top {} tokens are saved'.format(common_words + len(special_tokens)))\n",
    "            \n",
    "            \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi . ', 'hi . ', 'run ! ', 'wow ! ', 'wow ! ', 'fire ! ', 'help ! ', 'help ! ', 'stop ! ', 'wait ! ']\n",
      "['hallo ! <EOS> ', 'gru gott ! <EOS> ', 'lauf ! <EOS> ', 'potzdonner ! <EOS> ', 'donnerwetter ! <EOS> ', 'feuer ! <EOS> ', 'hilfe ! <EOS> ', 'zu hulf ! <EOS> ', 'stopp ! <EOS> ', 'warte ! <EOS> ']\n"
     ]
    }
   ],
   "source": [
    "path = 'deu.txt'\n",
    "load_file = LoadData(path, pair=True)\n",
    "pair_language = load_file.load_text()\n",
    "english_sentences, german_sentences = load_file.clean_data()\n",
    "print(english_sentences[: 10])\n",
    "print(german_sentences[: 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "all sentences are saved !\n",
      "2\n",
      "all sentences are saved !\n",
      "2\n",
      "file saved\n"
     ]
    }
   ],
   "source": [
    "load_file.save_txt(english_sentences, 'words/eng_sentences.txt')\n",
    "load_file.save_txt(german_sentences, 'words/german_sentences.txt')\n",
    "load_file.save_binary(english_sentences, german_sentences, 'words/ger_eng.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi . ', 'hi . ', 'run ! ', 'wow ! ', 'wow ! ', 'fire ! ', 'help ! ', 'help ! ', 'stop ! ', 'wait ! ']\n",
      "['hallo ! <EOS> ', 'gru gott ! <EOS> ', 'lauf ! <EOS> ', 'potzdonner ! <EOS> ', 'donnerwetter ! <EOS> ', 'feuer ! <EOS> ', 'hilfe ! <EOS> ', 'zu hulf ! <EOS> ', 'stopp ! <EOS> ', 'warte ! <EOS> ']\n"
     ]
    }
   ],
   "source": [
    "eng, ger = load_file.load_binary('words/ger_eng.p')\n",
    "\n",
    "print(eng[: 10])\n",
    "print(ger[: 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens are counted !\n",
      "tokens are counted !\n",
      "15104\n",
      "2\n",
      "all 15107 tokens are saved\n",
      "33073\n",
      "2\n",
      "all 33074 tokens are saved\n"
     ]
    }
   ],
   "source": [
    "# count tokens\n",
    "load_file.count_tokens(english_sentences)\n",
    "load_file.count_tokens(german_sentences, src=False)\n",
    "# save tokens in a file\n",
    "print(len(load_file._src_counter))\n",
    "load_file.save_tokens('words/eng_words.txt')\n",
    "print(len(load_file._target_counter))\n",
    "load_file.save_tokens('words/ger_words.txt', src_counter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john', 'lim', 'kim.txt']\n"
     ]
    }
   ],
   "source": [
    "list_phrases = [\"hello\", \"go\"]\n",
    "path_test =\"john/lim/kim.txt\"\n",
    "split_path = path_test.split(\"/\")\n",
    "print(split_path)\n",
    "if(os.path.exists(\"/\".join(split_path[:- 1])) == False):\n",
    "    os.makedirs(\"/\".join(split_path[:- 1]))\n",
    "with open(path_test, 'w') as saved_file:\n",
    "    \n",
    "            for phrase in list_phrases:\n",
    "                print(phrase, file=saved_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi . ', 'hi . ', 'run ! ', 'wow ! ', 'wow ! ', 'fire ! ', 'help ! ', 'help ! ', 'stop ! ', 'wait ! ']\n",
      "192881\n",
      "****************************************\n",
      "['hallo ! <EOS> ', 'gru gott ! <EOS> ', 'lauf ! <EOS> ', 'potzdonner ! <EOS> ', 'donnerwetter ! <EOS> ', 'feuer ! <EOS> ', 'hilfe ! <EOS> ', 'zu hulf ! <EOS> ', 'stopp ! <EOS> ', 'warte ! <EOS> ']\n",
      "192881\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "with open(\"all_data.p\", 'rb') as file:\n",
    "    source_data, target_data = pickle.load(file)\n",
    "\n",
    "print(source_data[: 10])\n",
    "print(len(source_data))\n",
    "\n",
    "print(\"**\" * 20)\n",
    "print(target_data[: 10])\n",
    "print(len(target_data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi . ', 'hi . ', 'run ! ', 'wow ! ', 'wow ! ', 'fire ! ', 'help ! ', 'help ! ', 'stop ! ', 'wait ! ']\n",
      "192881\n",
      "****************************************\n",
      "['hallo ! <EOS> ', 'gru gott ! <EOS> ', 'lauf ! <EOS> ', 'potzdonner ! <EOS> ', 'donnerwetter ! <EOS> ', 'feuer ! <EOS> ', 'hilfe ! <EOS> ', 'zu hulf ! <EOS> ', 'stopp ! <EOS> ', 'warte ! <EOS> ']\n",
      "192881\n"
     ]
    }
   ],
   "source": [
    "with open('words/ger_eng.p', 'rb') as file:\n",
    "    ssource_data, ttarget_data = pickle.load(file)\n",
    "\n",
    "print(ssource_data[: 10])\n",
    "print(len(ssource_data))\n",
    "print(\"**\" * 20)\n",
    "print(ttarget_data[: 10])\n",
    "print(len(ttarget_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
