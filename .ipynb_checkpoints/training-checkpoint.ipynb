{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from simple_model import BasicModel\n",
    "from robot import DataFlow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.01\n",
    "keep_probability = 0.8\n",
    "unit_size = 128\n",
    "enc_embed_dim = 256\n",
    "dec_embed_dim = 256\n",
    "batch_size = 32\n",
    "num_layers = 3\n",
    "data_filename = 'words/eng_finn.p\n",
    "source_words_filename = 'words/english_words.txt'\n",
    "target_words_filename = 'words/finn_words.txt'\n",
    "percent_split_data = 0.9\n",
    "# model_data = DataFlow(data_filename, source_words_filename, target_words_filename, percent_split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model \n",
    "# basic_model = BasicModel(model_data, unit_size, learning_rate, keep_probability, batch_size, num_layers,\n",
    "#                          enc_embed_dim, dec_embed_dim)\n",
    "# print('the basic model has been created successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataflow object has been created successfully !\n",
      "the basic model has been created successfully!\n",
      " encoder number of tokens :\n",
      " num encoder tokens: 15106\n",
      "WARNING:tensorflow:From /home/mehdi/Desktop/jub2/ger_translation/simple_model.py:34: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "num decoder tokens: 33039\n",
      " we are in the training phase !\n",
      "congratulations the decoder train phase passed the test !!\n",
      "we finished the training phase and we reached the inference phase !\n",
      "congratulations the inference phase passed the test successfully !!!\n",
      "congratulations ! the training mode is working perfectly !!\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    model_data = DataFlow(data_filename, source_words_filename, target_words_filename, \n",
    "                          percent_split_data, batch_size)\n",
    "    basic_model = BasicModel(model_data, unit_size, learning_rate, keep_probability, batch_size, num_layers,\n",
    "                         enc_embed_dim, dec_embed_dim)\n",
    "    print('the basic model has been created successfully!')\n",
    "    \n",
    "    train_data, validation_data = basic_model.prepare_train_validation_data()\n",
    "    # iterator to switch between the training data and the validation data\n",
    "    iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes)\n",
    "    (source_data, source_lengths), (target_data,  target_lengths) = iterator.get_next()\n",
    "    # getting the logits and predictions\n",
    "    logits, predictions = basic_model.model(source_data, \n",
    "                                            source_lengths, \n",
    "                                            target_data, \n",
    "                                            target_lengths, \n",
    "                                            mode='train')\n",
    "    # training phase and getting the loss\n",
    "    loss, train_op = basic_model.optimizer(logits, target_data, target_lengths)\n",
    "    print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start !!\n"
     ]
    }
   ],
   "source": [
    "# training session initialize all the needed inits and variables\n",
    "train_init = iterator.make_initializer(train_data)\n",
    "validation_init = iterator.make_initializer(validation_data)\n",
    "init_variables = tf.global_variables_initializer()\n",
    "init_tables = tf.tables_initializer()\n",
    "epochs = 100\n",
    "train_steps = basic_model._data_flow.nb_train_batches\n",
    "validation_steps = basic_model._data_flow.nb_val_batches\n",
    "checkpoints = 'checkpoints/saved_weights.ckpt'\n",
    "\n",
    "\n",
    "\n",
    "save_path = 'model'\n",
    "# start the training\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    train_epochs_losses = []\n",
    "    validation_epochs = []\n",
    "    print('start !!')\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        sess.run(train_init)\n",
    "        \n",
    "        for mini_batch in range(1, train_steps + 1):\n",
    "            # make the training and get the loss and the predictions of the training data\n",
    "            train_loss, _, train_predictions = sess.run([loss, train_op, predictions])\n",
    "            print(train_predictions)\n",
    "            print('epoch: {} batch: {}/{}\\tloss: {}'.format(epoch, batch_size * mini_batch, batch_size * train_steps, train_loss))\n",
    "            if mini_batch == train_steps:\n",
    "                print('', end='\\t')\n",
    "            \n",
    "            \n",
    "        train_epochs_losses.append(train_loss) # append all the last lossess of all epochs \n",
    "        sess.run(validation_init)   # swap to the validation data\n",
    "        validation_losses = []\n",
    "        for _ in range(1, validation_steps + 1):\n",
    "            # get the predictions and losses of the validation data\n",
    "            loss_validation, predictions_val = sess.run([loss, predictions])\n",
    "            validation_losses.append(loss_val)\n",
    "        # calculate the loss mean of all the validation data mini batches    \n",
    "        validation_loss = np.mean(validation_losses)\n",
    "        print('validation loss: {:>4.f4}'.format(validation_loss))\n",
    "        print('--' * 20)\n",
    "        if epoch % 5 == 0:\n",
    "            print('saving the brain')\n",
    "            saver.save(sess, checkpoints)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
